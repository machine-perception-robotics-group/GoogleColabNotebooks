{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回帰結合型のニューラルネットワーク（GRU）による電力予測\n",
    "---\n",
    "\n",
    "## 目的\n",
    "Gated recurrent unit (GRU) を用いて，電力予測の実験を行なう．\n",
    "\n",
    "\n",
    "## 対応するチャプター\n",
    "* 10.2: 回帰結合型ネットワークにおける勾配計算（BPTT）\n",
    "* 10.10.2: GRU\n",
    "* 10.11: 勾配のクリッピング\n",
    "\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのダウンロード\n",
    "プログラムの動作に必要なデータをダウンロードし，zipファイルを解凍します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://www.mprg.cs.chubu.ac.jp/tutorial/ML_Lecture/SOLAR/data.zip -O data.zip\n",
    "!unzip -q -o data.zip\n",
    "!ls -R ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n",
    "読み込んだデータを変換します．\n",
    "ここで，delayは何時刻先の電力値を教師信号にするかを決定するためのパラメータです．\n",
    "delay=1と設定した場合，ネットワークへ入力したデータの1時刻先の電力が正解ラベルとなります．\n",
    "\n",
    "データのサイズを確認します．\n",
    "ネットワークへの入力データサイズは34となっており，時刻や曜日などの情報を表現したデータとなっています，出力の値は対応する電力値の1つとなっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_x = np.load('./data/train/BEMS_RNN_train_data.npy')\n",
    "train_y = np.load('./data/train/BEMS_RNN_train_labels.npy')\n",
    "test_x  = np.load('./data/test/BEMS_RNN_test_data.npy')\n",
    "test_y = np.load('./data/test/BEMS_RNN_test_labels.npy')\n",
    "\n",
    "# 0~1に収まるように正規化\n",
    "train_x = (train_x - (-0.04)) / (1.3 - (-0.04))\n",
    "train_y = (train_y - (-0.04)) / (1.3 - (-0.04))\n",
    "test_x = (test_x - (-0.04)) / (1.3 - (-0.04))\n",
    "test_y = (test_y - (-0.04)) / (1.3 - (-0.04))\n",
    "\n",
    "# 数時刻先の電力が正解データになるように変換\n",
    "delay = 1\n",
    "train_x = np.asarray(train_x[ : -delay])\n",
    "train_y = np.asarray(train_y[delay : ])\n",
    "test_x = np.asarray(test_x[ : -delay])\n",
    "test_y = np.asarray(test_y[delay : ])\n",
    "\n",
    "# データのサイズ確認\n",
    "train_y = train_y.reshape(len(train_y), 1)\n",
    "test_y = test_y.reshape(len(test_y), 1)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "次にネットワーク（LSTM）を定義します．\n",
    "\n",
    "まずはじめに，ネットワークの定義に必要な関数を定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "#     return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    return (1.0 - x) * x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "#     return 1.0 - np.tanh(x) * np.tanh(x)\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def fc(x, w, b):\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "def fc_grad(dout, x, w, b):\n",
    "    dx = np.dot(dout, w.T)\n",
    "    d_w = np.dot(x.T, dout)\n",
    "    d_b = np.sum(dout, axis=0)\n",
    "    return dx, d_w, d_b\n",
    "\n",
    "def mean_squared_error(pred, true):\n",
    "    return np.sum(np.power(pred - true, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，上で定義した関数を用いてネットワークを定義します．\n",
    "ここでは，GRU，出力層から構成されるネットワークとします．\n",
    "\n",
    "入力層と中間層，出力層のユニット数は引数として与え，それぞれ`n_input`，`n_hidden`, `n_out`とします．\n",
    "そして，`__init__`関数を用いて，ネットワークのパラメータを初期化します．\n",
    "下記のプログラムのうち，`w_z`から`b_h`はLSTMが持つパラメータを表しています．\n",
    "`w`と`b`はそれぞれ出力層の重みとバイアスを表しています．\n",
    "重みは`randn`関数で，標準正規分布に従った乱数で生成した値を保有する配列を生成します．\n",
    "バイアスは`zeros`関数を用いて，要素が全て0の配列を生成します．\n",
    "\n",
    "そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n",
    "\n",
    "次に，`backward`関数ではパラメータの更新量を計算します．\n",
    "まず，ネットワークの出力結果と教師ラベルから，誤差`dy`を算出します．\n",
    "この時，教師ラベルをone-hotベクトルへ変換し，各ユニットの出力との差を取ることで，`dy`を計算しています．\n",
    "その後，連鎖律に基づいて，出力層から順番に勾配を計算していきます．\n",
    "このとき，パラメータの更新量を`self.grads`へ保存しておきます．\n",
    "また，LSTMでは時刻間で隠れ層に対する勾配を伝播する必要があるため，\n",
    "`backward`の引数として，1時刻先からの勾配情報を受け取り，前の時刻へ渡す勾配情報を返すようにしています．\n",
    "\n",
    "最後に`update_parameters`関数で，更新量をもとにパラメータの更新を行います．\n",
    "\n",
    "また，各時刻で求めた勾配を累積する必要があるため，`clear_grads`関数で，各パラメータの勾配情報を初期化する関数を定義し，学習時に使用します．\n",
    "\n",
    "`clip_gradients`関数は，獲得した勾配を指定した範囲に収める関数です．\n",
    "LSTMでは，各時刻の勾配を累積するため，勾配量が大きくなり勾配消失や爆発が起きる可能性があります．\n",
    "ここでは，指定した範囲内に勾配を調整し，パラメータを更新することで，消失や爆発を抑制します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, w_std=0.01):\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # GRU params\n",
    "        self.w_z = np.random.randn(n_input, n_hidden) * w_std\n",
    "        self.u_z = np.random.randn(n_hidden, n_hidden) * w_std\n",
    "        self.b_z = np.zeros(n_hidden)\n",
    "        \n",
    "        self.w_r = np.random.randn(n_input, n_hidden) * w_std\n",
    "        self.u_r = np.random.randn(n_hidden, n_hidden) * w_std\n",
    "        self.b_r = np.zeros(n_hidden)\n",
    "        \n",
    "        self.w_h = np.random.randn(n_input, n_hidden) * w_std\n",
    "        self.u_h = np.random.randn(n_hidden, n_hidden) * w_std\n",
    "        self.b_h = np.zeros(n_hidden)\n",
    "        \n",
    "        # output layer params\n",
    "        self.w = np.random.randn(n_hidden, n_out) * w_std\n",
    "        self.b = np.zeros(n_out)\n",
    "\n",
    "        self.grads = {}\n",
    "        self.clear_grads()\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # update gate\n",
    "        self.z = sigmoid(np.dot(x, self.w_z) + np.dot(h_prev, self.u_z) + self.b_z)\n",
    "        \n",
    "        # reset gate\n",
    "        self.r = sigmoid(np.dot(x, self.w_r) + np.dot(h_prev, self.u_r) + self.b_r)\n",
    "        \n",
    "        # hidden state\n",
    "        self.h_hat = tanh(np.dot(x, self.w_h) + \n",
    "                          np.dot(np.multiply(self.r, h_prev), self.u_h) + self.b_h)\n",
    "        self.h = np.multiply((1 - self.z), h_prev) + np.multiply(self.z, self.h_hat)\n",
    "        \n",
    "        # output\n",
    "        self.y = np.dot(self.h, self.w) + self.b\n",
    "\n",
    "        variables = {'x': x, 'h_prev': h_prev, 'z': self.z, 'r': self.r,\n",
    "                     'h_hat': self.h_hat, 'h': self.h, 'y': self.y}\n",
    "\n",
    "        return variables, self.h\n",
    "    \n",
    "    def backward(self, in_var, t, d_h_next):\n",
    "        batch_size = in_var['x'].shape[0]\n",
    "        y = sigmoid(in_var['y'])\n",
    "        \n",
    "        # output (MSE, sigmoid)\n",
    "        dy = (t - y) / batch_size\n",
    "        d_out = sigmoid_grad(y) * dy\n",
    "        \n",
    "        # output layer\n",
    "        self.grads['w'] += np.dot(in_var['h'].T, d_out)\n",
    "        self.grads['b'] += np.sum(d_out, axis=0)\n",
    "        \n",
    "        d_h = np.dot(d_out, self.w.T) + d_h_next\n",
    "        d_h_hat = np.multiply(d_h, (1 - in_var['z']))\n",
    "        d_h_hat_l = tanh_grad(in_var['h_hat']) * d_h_hat\n",
    "        \n",
    "        # hidden state\n",
    "        self.grads['w_h'] += np.dot(in_var['x'].T, d_h_hat_l)\n",
    "        self.grads['u_h'] += np.dot(np.multiply(in_var['r'], in_var['h_prev']).T, d_h_hat_l)\n",
    "        self.grads['b_h'] += np.sum(d_h_hat_l, axis=0)\n",
    "        \n",
    "        d_r_hp = np.dot(d_h_hat_l, self.u_h.T)\n",
    "        d_r = np.multiply(d_r_hp, in_var['h_prev'])\n",
    "        d_r_l = sigmoid_grad(in_var['r']) * d_r\n",
    "        \n",
    "        # reset gate\n",
    "        self.grads['w_r'] += np.dot(in_var['x'].T, d_r_l)\n",
    "        self.grads['u_r'] += np.dot(in_var['h_prev'].T, d_r_l)\n",
    "        self.grads['b_r'] += np.sum(d_r_l, axis=0)\n",
    "        \n",
    "        d_z = np.multiply(d_h, in_var['h_prev'] - in_var['h_hat'])\n",
    "        d_z_l = d_z * sigmoid_grad(in_var['z'])\n",
    "        \n",
    "        # update gate\n",
    "        self.grads['w_z'] += np.dot(in_var['x'].T, d_z_l)\n",
    "        self.grads['u_z'] += np.dot(in_var['h_prev'].T, d_z_l)\n",
    "        self.grads['b_z'] += np.sum(d_z_l, axis=0)\n",
    "        \n",
    "        d_h_z_inner = np.dot(d_z_l, self.u_z.T)\n",
    "        d_h_z = np.multiply(d_h, in_var['z'])\n",
    "        d_h_h = np.multiply(d_r_hp, in_var['r'])\n",
    "        d_h_r = np.dot(d_r_l, self.u_r.T)\n",
    "        d_h_prev = d_h_z_inner + d_h_z + d_h_h + d_h_r\n",
    "        \n",
    "        return d_h_prev\n",
    "        \n",
    "    def update_parameters(self, lr=0.01):\n",
    "        self.w_z += lr * self.grads['w_z']\n",
    "        self.u_z += lr * self.grads['u_z']\n",
    "        self.b_z += lr * self.grads['b_z']\n",
    "        self.w_r += lr * self.grads['w_r']\n",
    "        self.u_r += lr * self.grads['u_r']\n",
    "        self.b_r += lr * self.grads['b_r']\n",
    "        self.w_h += lr * self.grads['w_h']\n",
    "        self.u_h += lr * self.grads['u_h']\n",
    "        self.b_h += lr * self.grads['b_h']\n",
    "        self.w += lr * self.grads['w']\n",
    "        self.b += lr * self.grads['b']\n",
    "\n",
    "    def clear_grads(self):\n",
    "        self.grads = {'w_z': np.zeros(self.w_z.shape), 'u_z': np.zeros(self.u_z.shape), 'b_z': np.zeros(self.b_z.shape),\n",
    "                      'w_r': np.zeros(self.w_r.shape), 'u_r': np.zeros(self.u_r.shape), 'b_r': np.zeros(self.b_r.shape),\n",
    "                      'w_h': np.zeros(self.w_h.shape), 'u_h': np.zeros(self.u_h.shape), 'b_h': np.zeros(self.b_h.shape),\n",
    "                      'w': np.zeros(self.w.shape), 'b': np.zeros(self.b.shape)}\n",
    "\n",
    "    def clip_gradients(self, clip_val=1.0):\n",
    "        for k in self.grads.keys():\n",
    "            self.grads[k] = np.clip(self.grads[k], -clip_val, clip_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの変換\n",
    "\n",
    "上で読み込んだ学習・テストデータは電力の推移を表した１つの時系列データとなっているため，この一つの時系列データから，短い時間間隔で区切ったデータを作成することで，学習データの作成を行います．\n",
    "\n",
    "まず，time_windowで1サンプルの時間窓を決定します．\n",
    "今回は10時刻で1サンプルと設定します．\n",
    "その後，指定した時間窓でサンプルを抽出し，convert_train_x, とconvert_train_yへと保存することで，学習データを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 10\n",
    "\n",
    "convert_train_x = []\n",
    "convert_train_y = []\n",
    "for idx_frame in range(len(train_x) - time_window):\n",
    "    partial_data = train_x[idx_frame:idx_frame + time_window]\n",
    "    partial_label = train_y[idx_frame:idx_frame + time_window]\n",
    "\n",
    "    convert_train_x.append(partial_data)\n",
    "    convert_train_y.append(partial_label)\n",
    "\n",
    "convert_train_x = np.asarray(convert_train_x)\n",
    "convert_train_y = np.asarray(convert_train_y)\n",
    "\n",
    "print(convert_train_x.shape, convert_train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの作成と学習の準備\n",
    "上で定義したネットワークを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 34\n",
    "n_hidden = 128\n",
    "n_out = 1\n",
    "\n",
    "model = GRU(n_in, n_hidden, n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "学習を行います．\n",
    "\n",
    "1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を10とします．\n",
    "\n",
    "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x_batch`と`y_batch`とします．\n",
    "\n",
    "まず，LSTMに用いる，隠れ状態`h`およびセル状態`c`を初期化します．\n",
    "\n",
    "その後，`x_batch`に含まれる各時刻の入力データを順次LSTMへと入力します．\n",
    "この時，勾配計算のために，各時刻の出力を保存しておきます．\n",
    "\n",
    "その後，`backward`関数を用いて勾配を計算します．\n",
    "まず，1時刻先の勾配として`d_h_next`, `d_c_next`を初期化します．\n",
    "そして，勾配を計算する前に，`clear_grads`関数を用いて，累積する勾配情報を初期化します．\n",
    "その後，入力とは逆の順番にデータを取得し，backward関数を適用します．\n",
    "\n",
    "勾配の爆発を防ぐために，`clip_gradients`を用いて，一定量に収め，`update_parameters`でパラメータを更新します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = convert_train_x.shape[0]\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    \n",
    "    perm = np.random.permutation(train_data_num)\n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        x_batch = convert_train_x[perm[i:i+batch_size]]\n",
    "        y_batch = convert_train_y[perm[i:i+batch_size]]\n",
    "        \n",
    "        # forward\n",
    "        h = np.zeros((x_batch.shape[0], n_hidden))\n",
    "\n",
    "        var_list = []\n",
    "        loss_tmp = 0.0\n",
    "        for i in range(time_window):\n",
    "            var_tmp, h = model.forward(x_batch[:, i, :], h)\n",
    "            loss_tmp += mean_squared_error(sigmoid(var_tmp['y']), y_batch[:, i, :])\n",
    "            var_list.append(var_tmp)\n",
    "\n",
    "        sum_loss  += loss_tmp / time_window\n",
    "\n",
    "        # backward\n",
    "        d_h_next = np.zeros_like(h)\n",
    "        model.clear_grads()\n",
    "        for i, var_tmp in enumerate(reversed(var_list)):\n",
    "            d_h_next = model.backward(var_tmp, y_batch[:, time_window - i - 1, :], d_h_next)\n",
    "        \n",
    "        model.clip_gradients(clip_val=1.0)\n",
    "        model.update_parameters(lr=0.01)\n",
    "        \n",
    "    print(\"epoch: {}, mean loss: {}\".format(epoch, sum_loss / train_data_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークを用いて，テストデータに対する出力結果の確認を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_test_x = test_x.reshape(1, 9999, 34)\n",
    "convert_test_y = test_y.reshape(1, 9999, 1)\n",
    "\n",
    "h = np.zeros((convert_test_x.shape[0], n_hidden))\n",
    "\n",
    "result = []\n",
    "for i in range(convert_test_x.shape[1]):\n",
    "    var_tmp, h = model.forward(convert_test_x[:, i, :], h)\n",
    "    result.append(sigmoid(var_tmp['y']))\n",
    "\n",
    "prediction_result = np.array(result).flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(test_y.tolist(), color='red', label='true')\n",
    "plt.plot(prediction_result.tolist(), color='blue', label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
