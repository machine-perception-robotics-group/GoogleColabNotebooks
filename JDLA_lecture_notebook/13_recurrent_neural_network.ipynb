{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回帰結合型のニューラルネットワークによる文章生成\n",
    "\n",
    "---\n",
    "## 目的\n",
    "回帰結合型のニューラルネットワーク，すなわち再帰型ニューラルネットワーク (Recurrent Neural Network; RNN) を用いてPenn Tree Bankデータセットに対する次単語の予測を行う．\n",
    "また，教師強制の有無による性能の違いを確認する．\n",
    "\n",
    "\n",
    "## 対応するチャプター\n",
    "* 10.2: 教師強制と出力回帰のあるネットワーク\n",
    "* 10.2: 回帰結合型ネットワークにおける勾配計算（BPTT）\n",
    "* 10.10.1: LSTM\n",
    "* 10.10.2: GRU\n",
    "* 10.11: 勾配のクリッピング\n",
    "\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import get_ptb_words, get_ptb_words_vocabulary\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.optimizer_hooks import GradientClipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`GPU avilability: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "\n",
    "Penn Tree Bank (PTB) データセットを読み込みます．\n",
    "\n",
    "読み込んだ学習データのサイズを確認します．\n",
    "学習，検証，テストデータはそれぞれ929589，73760，82430のサイズの1次元配列になっていることがわかります．\n",
    "\n",
    "また，`get_ptb_words_vocabulary`関数を用いて，ptbデータセットに存在する英単語の情報を取得します．\n",
    "`vocab`には英単語とその単語を示すIDが辞書型のオブジェクトとして格納されています．\n",
    "英単語の数は10000です．\n",
    "\n",
    "最後に，keyと値の組み合わせを逆にした辞書`inverse_vocab`を作成します．\n",
    "これはIDで出力された予測結果から英単語を検索する際に使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "train, val, test = get_ptb_words()\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "# 単語（vocabulary）の確認\n",
    "vocab = get_ptb_words_vocabulary()\n",
    "print(len(vocab))\n",
    "\n",
    "# 逆引きの辞書を作成\n",
    "inverse_vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benn Tree Bankデータセットの表示\n",
    "\n",
    "PTBデータセットの中身を`print`関数を使って表示してみます．\n",
    "\n",
    "学習用データを表示すると，1次元配列に整数値が格納されていることがわかります．\n",
    "\n",
    "また，`vocab`のうち，英単語を指定すると，各英単語に対応するIDガ表示されます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train sentence:\", train)\n",
    "print(vocab['player'], vocab['primarily'], vocab['arose'], vocab['generate'], vocab['partnership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "再帰型ニューラルネットワークを定義します．\n",
    "\n",
    "ここでは，埋め込み層1層，LSTM層1層，全結合層1層から構成されるネットワークとします．\n",
    "\n",
    "`reset_state`関数では，LSTM層が持つ，内部状態（隠れ状態・セル状態）を初期化します．\n",
    "\n",
    "次に，`__call__`関数では，定義した層を接続して処理するように記述します．\n",
    "`__call__`関数の引数`x`は入力データ（単語のID）です．\n",
    "入力データは`embed`にて，入力された単語のIDから入力された単語を表現するベクトルを生成します．\n",
    "その後，LSTM，全結合層へと入力することで，入力された単語の次の単語を予測結果として出力します．\n",
    "その際，LSTMおよび全結合層からの出力にはdropoutを適用しており，過学習の抑制を図っています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units):\n",
    "        super(RNNLM, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed = L.EmbedID(n_vocab, n_units)\n",
    "            self.l1 = L.LSTM(n_units, n_units)\n",
    "            self.l2 = L.Linear(n_units, n_vocab)\n",
    "\n",
    "        for param in self.params():\n",
    "            param.array[...] = np.random.uniform(-0.1, 0.1, param.shape)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        y = self.l2(F.dropout(h1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Dwuvfouzmd7"
   },
   "source": [
    "## ネットワークの作成\n",
    "上のプログラムで定義したネットワークを作成します．\n",
    "ここでは，GPUで学習を行うために，modelをGPUに送るto_gpu関数を利用しています．\n",
    "\n",
    "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を1.0として引数に与えます．そして，最適化方法のsetup関数にネットワークモデルを与えます．\n",
    "\n",
    "また，勾配の爆発により学習の不安定性に対応するため，勾配のクリッピングを行います．\n",
    "最適化手法を設定した`optimizer_1`に`add_hook`メソッドを用いて学習を行う際の条件を追加します．\n",
    "ここでは，勾配のクリッピングを行う`GradientClipping`関数を追加します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1562840288195,
     "user": {
      "displayName": "Tsubasa Hirakawa",
      "photoUrl": "https://lh5.googleusercontent.com/-p6Kjr3nd0AU/AAAAAAAAAAI/AAAAAAAAJG0/tCF9JFOo7tk/s64/photo.jpg",
      "userId": "03545166870843244307"
     },
     "user_tz": -540
    },
    "id": "23m79Eq-zmjl",
    "outputId": "3973baec-fcdd-4766-e766-49da6921010f"
   },
   "outputs": [],
   "source": [
    "num_vocab = len(vocab)\n",
    "num_units = 1024\n",
    "model_1 = RNNLM(n_vocab=num_vocab, n_units=num_units)\n",
    "model_1.to_gpu()\n",
    "\n",
    "optimizer_1 = chainer.optimizers.MomentumSGD(lr=1.0, momentum=0.9)\n",
    "optimizer_1.setup(model_1)\n",
    "optimizer_1.add_hook(GradientClipping(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，GPUに対応した行列演算モジュールのcupyを呼び出し，学習およびテストデータをcupyの形式に変換します．\n",
    "cupyはnumpyと互換性があります．\n",
    "\n",
    "先ほど読み込んだPTBデータセットを学習に使用するために，データを整理します．\n",
    "まず`bproplen`でネットワークへ入力するデータの長さを指定します．\n",
    "その後，学習データを指定した長さに区切ることで，学習サンプルを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.cupy\n",
    "\n",
    "bproplen = 35\n",
    "\n",
    "train_x, train_y = [], []\n",
    "for idx_window in range(0, len(train) - bproplen - 1, 10):\n",
    "    train_x.append(train[idx_window:idx_window + bproplen])\n",
    "    train_y.append(train[idx_window + 1:idx_window + bproplen + 1])\n",
    "train_x = xp.array(train_x, dtype=xp.int32)\n",
    "train_y = xp.array(train_y, dtype=xp.int32)\n",
    "\n",
    "val = xp.array(val, dtype=xp.int32)\n",
    "test = xp.array(test, dtype=xp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習（教師強制; Teacher forcing）\n",
    "\n",
    "教師強制の方法でネットワークを学習します．\n",
    "教師強制ではネットワークへの入力データとして，教師データ（正しい英単語）を順番に入力し，出力と教師ラベルとの誤差を用いて学習する方法です．\n",
    "\n",
    "１回の誤差を算出するデータ数（ミニバッチサイズ）128，学習エポック数を100とします．\n",
    "先ほど作成したの学習データサイズを取得し，1エポック内における更新回数を求めます．\n",
    "学習データは毎エポックでランダムに利用するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x`と`t`とし，`to_gpu`関数でGPUに転送します．\n",
    "学習モデルにxを与えて各クラスの確率`y`を取得します．\n",
    "各クラスの確率`y`と教師ラベル`t`との誤差を`softmax_coross_entropy`誤差関数で算出します．\n",
    "そして，誤差を`backward`関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "train_data_num = train_x.shape[0]\n",
    "num_iter_per_epoch = int(train_data_num / batch_size)\n",
    "\n",
    "\n",
    "# 学習の実行\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    perm = xp.random.permutation(train_data_num)\n",
    "    \n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        \n",
    "        x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "        t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "        \n",
    "        accum_loss = 0\n",
    "        model_1.reset_state()\n",
    "        \n",
    "        for idx_window in range(bproplen):\n",
    "            \n",
    "            y = model_1(x[:, idx_window])\n",
    "            \n",
    "            loss = F.softmax_cross_entropy(y, t[:, idx_window])\n",
    "            accum_loss += loss\n",
    "            sum_loss += loss.data\n",
    "\n",
    "        optimizer_1.target.cleargrads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()\n",
    "        optimizer_1.update()\n",
    "\n",
    "    elapsed_time = time() - start\n",
    "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
    "                                                              sum_loss/num_iter_per_epoch,\n",
    "                                                              elapsed_time))\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        model.to_cpu()\n",
    "        chainer.serializers.save_npz(\"rnn-%03d.npz\" % epoch, model)\n",
    "        model.to_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習（教師強制を用いない学習）\n",
    "\n",
    "こちらでは，教師強制を用いない学習を行います．\n",
    "この方法は，ネットワークから出力された結果（予測された英単語）を次の入力として順番に使用し学習する方法です．\n",
    "\n",
    "\n",
    "まず，先ほどとは異なるネットワークとして`model_2`を作成し，最適化手法を設定します．\n",
    "この時のパラメータは同じものを使用します．\n",
    "\n",
    "最初の入力のみ`x`すなわち正しい単語を入力し，それ以外の入力には前の時刻の結果`y`から求められた単語を入力します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの作成\n",
    "model_2 = RNNLM(n_vocab=num_vocab, n_units=num_units)\n",
    "model_2.to_gpu()\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer_2 = chainer.optimizers.MomentumSGD(lr=1.0, momentum=0.9)\n",
    "optimizer_2.setup(model_2)\n",
    "optimizer_2.add_hook(GradientClipping(5.0))\n",
    "\n",
    "\n",
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "train_data_num = train_x.shape[0]\n",
    "num_iter_per_epoch = int(train_data_num / batch_size)\n",
    "\n",
    "\n",
    "# 学習の実行\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    perm = xp.random.permutation(train_data_num)\n",
    "    \n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        \n",
    "        x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "        t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "\n",
    "        pred = None\n",
    "        accum_loss = 0\n",
    "        model_2.reset_state()\n",
    "\n",
    "        for idx_window in range(bproplen):\n",
    "            \n",
    "            if idx_window == 0:\n",
    "                y = model_2(x[:, idx_window])\n",
    "            else:\n",
    "                y = model_2(pred)\n",
    "                    \n",
    "            pred = F.argmax(y, axis=1)\n",
    "            \n",
    "            loss = F.softmax_cross_entropy(y, t[:, idx_window])\n",
    "            accum_loss += loss\n",
    "            sum_loss += loss.data\n",
    "            \n",
    "        optimizer_2.target.cleargrads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()\n",
    "        optimizer_2.update()\n",
    "            \n",
    "    elapsed_time = time() - start\n",
    "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
    "                                                              sum_loss/num_iter_per_epoch,\n",
    "                                                              elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークモデルを用いて評価を行います．\n",
    "\n",
    "\n",
    "### 1. 教師強制ありのモデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentense = []\n",
    "pred_sentense = []\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    pred_word = None\n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_1(x)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "    \n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_1(pred)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "\n",
    "print(' '.join(true_sentense[0:10]))\n",
    "print(' '.join(pred_sentense[9:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 教師強制なしのモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentense = []\n",
    "pred_sentense = []\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    pred_word = None\n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_2(x)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "    \n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_2(pred)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "\n",
    "print(' '.join(true_sentense[0:10]))\n",
    "print(' '.join(pred_sentense[9:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "1. ネットワークのLSTM層の数を変更した際の性能変化を確認しましょう\n",
    "2. ネットワークのLSTMをGRUに変更して性能の変化を確認しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
