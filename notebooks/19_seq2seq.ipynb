{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19_seq2seq.ipynb","provenance":[{"file_id":"https://github.com/machine-perception-robotics-group/GoogleColabNotebooks/blob/master/notebooks/19_seq2seq.ipynb","timestamp":1612764073741}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"G2BhrFGqFx_M"},"source":["# エンコーダ・デコーダによる計算機作成\n","\n","## 目的\n","再帰型ニューラルネットワークの構造を理解する\n","\n","エンコーダ・デコーダモデルの構造を理解する\n","\n","\n","## エンコーダ・デコーダモデル\n","\n","リカレントニューラルネットワークは，系列データ内の関連性を内部状態として保持することができます．\n","この内部状態を利用して，新たな出力ができるようにした構造としてエンコーダ・デコーダがあります．\n","エンコーダ側に系列データを入力して，中間層では系列データ内の関連性を内部状態を形成します．\n","デコーダ側には内部状態を与えることで，内部状態を反映した何かしらの結果を出力します．\n","この応用が，google 翻訳などの機械翻訳です．\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1zFl4Mjo4IRSQWSczJ4PzPkd53YJkb1oM\" width = 100%>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R-IW9H7q2fT7"},"source":["## モジュールのインポートとGPUの確認\n","\n","必要なモジュールをインポートします．\n","そして，GPUが使用可能かどうかを確認します．"]},{"cell_type":"code","metadata":{"id":"4ksMLJ652faP"},"source":["import random\n","import numpy as np\n","from time import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# GPUの確認\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvQCLiV4F1T6"},"source":["## データセットの作成\n","\n","文字とそれに対応したIDを整理した辞書型オブジェクトを作成します．\n","\n","`word2id`では文字をキーとしてIDをデータにもつ辞書を，一方`id2word`ではIDをキーとして文字列をデータに持つ辞書を作成します．\n","\n","作成した辞書を表示します．\n","`word2id`では，0から9の文字は0~9の数字のキーに対応しており，\n","各種記号は次のようなIDに対応しています．\n","* `<pad>`：10\n","* `+`：11\n","* 文字列の終わり`<eos>`：12\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5Xo3_CHplFN4"},"source":["word2id = {str(i): i for i in range(10)}\n","word2id.update({\"<pad>\": 10, \"+\": 11, \"<eos>\": 12})\n","id2word = {v: k for k, v in word2id.items()}\n","\n","# 作成した辞書オブジェクトの表示\n","print(word2id)\n","print(id2word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXKCw7yhlGTI"},"source":["### データセットクラスの作成\n","\n","次に，データセットを用意します．\n","\n","データは0から9までの数字と加算記号，開始，終了のフラグです．\n","また，３桁の数字の足し算を行うため，各桁の値を１つずつランダムに生成して連結しています．\n","\n","$$x + y = z$$\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5wUc1QN0Fygb"},"source":["class CalcDataset(torch.utils.data.Dataset):\n","\n","    # 計算式の文字列をIDに変換するための関数\n","    def transform(self, string, seq_len=7):\n","        tmp = []\n","        for i, c in enumerate(string):\n","            try:\n","                tmp.append(word2id[c])\n","            except:\n","                tmp += [word2id[\"<pad>\"]] * (seq_len - i)\n","                break\n","        return tmp\n","\n","    def __init__(self, data_num, train=True):\n","        super().__init__()\n","\n","        self.data_num = data_num\n","        self.train = train\n","        self.data = []\n","        self.label = []\n","\n","        for _ in range(data_num):\n","            x = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n","            y = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n","            left = (\"{:*<7s}\".format(str(x) + \"+\" + str(y))).replace(\"*\", \"<pad>\")\n","            self.data.append(self.transform(left))\n","\n","            z = x + y\n","            right = (\"{:*<6s}\".format(str(z))).replace(\"*\", \"<pad>\")\n","            right = self.transform(right, seq_len=5)\n","            right = [12] + right\n","            right[right.index(10)] = 12\n","            self.label.append(right)\n","        \n","        self.data = np.asarray(self.data)\n","        self.label = np.asarray(self.label)\n","\n","    def __getitem__(self, item):\n","        d = self.data[item]\n","        l = self.label[item]\n","        return d, l\n","\n","    def __len__(self):\n","        return self.data.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvI-V5Dndj3A"},"source":["### 作成したデータの確認\n","\n","作成した`CalcDataset`のデータを確認します．\n","\n","適当なデータセットとして`tmp_dataset`を作成します．今回はデータの確認を行うだけのため，データ数（`data_num`）は5と小さい数に指定します．\n","\n","そして，作成したデータセット内のデータをひとつづつfor文で読み出して，データを確認します．"]},{"cell_type":"code","metadata":{"id":"MJzPv5j7zWD9"},"source":["tmp_dataset = CalcDataset(data_num=5)\n","\n","for i in range(len(tmp_dataset)):\n","    print(tmp_dataset[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTHnQ96hF7cd"},"source":["## ネットワークモデル（計算機）の定義\n","ここでは，エンコーダ・デコーダ構造で計算機（足し算）を作ってみます．\n","このエンコーダ・デコーダ構造のことをSequence-to-Sequence (Seq2Seq) と呼びます．\n","\n","エンコーダとデコーダの2種類のネットワークを用意します．\n","エンコーダは，ワードエンベディング (word embedding) という入力されたIDを特徴表現に変換する層とLSTM層から構成されています．\n","デコーダも同様の構造です．エンコーダ側の中間層の値がstateとして出力され，デコーダ側の中間層に入力されます．"]},{"cell_type":"code","metadata":{"id":"_0IE4XYHF690"},"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n","        super(Encoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","\n","    def forward(self, indices):\n","        embedding = self.word_embeddings(indices)\n","        if embedding.dim() == 2:\n","            embedding = torch.unsqueeze(embedding, 1)\n","        h = torch.zeros(1, self.batch_size, self.hidden_dim, device=device)\n","        c = torch.zeros(1, self.batch_size, self.hidden_dim, device=device)\n","        _, state = self.lstm(embedding, (h, c))\n","        return state\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n","        super(Decoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.output = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, index, state):\n","        embedding = self.word_embeddings(index)\n","        if embedding.dim() == 2:\n","            embedding = torch.unsqueeze(embedding, 1)\n","        lstm_out, state = self.lstm(embedding, state)\n","        output = self.output(lstm_out)\n","        return output, state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-cRJNLIBKOs"},"source":["## ネットワークモデルの作成\n","\n","上で定義したエンコーダとデコーダを作成します．\n","エンコーダとデコーダは別々のネットワークとして用意し，それぞれの最適化にはAdamを利用します．"]},{"cell_type":"code","metadata":{"id":"DpsqOdLnBKUf"},"source":["embedding_dim = 16\n","hidden_dim = 128\n","vocab_size = len(word2id)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n","decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"<pad>\"])\n","\n","# 最適化手法の設定\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jPYDEx8F-d6"},"source":["###学習\n","学習を行います．学習データを2万サンプル生成して，データローダに与えます．\n","学習は200エポック行います．エンコーダの入力は数字または開始・終了・加算記号です．\n","デコーダの入力は計算結果です．\n","具体的には，54+37 を行う時，\n","エンコーダには，まず開始記号を最初に入力し，次に，5, 4, +, 3, 7 を入力します．そして，最後に終了記号を入力します．その時の中間層の情報をhidden_stateとしてエンコーダから受け取ります．\n","デコーダは，開始記号と中間情報(hidden_state)を最初に入力します，そして，計算結果の9, 1 を入力し，最後に終了記号を入力します．\n","この時，デコーダは各数字（または記号）の確率をdecoder_outputとして出力します．\n","decoder_outputは，[バッチサイズ, 1, 各クラス確率]の３次元なので，squeezeによって，[バッチサイズ,  各クラス確率] に次元削減します．\n","そして，クロスエントロピー誤差関数によって，ロスを求めます．\n","これを正解の長さ(=5)分繰り返し行い，ロスを累積します．\n","その後，誤差逆伝播，デコーダ，エンコーダの更新を行います．\n"]},{"cell_type":"code","metadata":{"id":"J6oQRPeTF-Bj"},"source":["batch_size=100\n","epoch_num = 200\n","\n","train_data = CalcDataset(data_num = 20000)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    for data, label in train_loader:\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        if use_cuda:\n","            data = data.cuda()\n","            label = label.cuda()\n","\n","        encoder_hidden = encoder(data)\n","        source = label[:, :-1]\n","        target = label[:, 1:]\n","        decoder_hidden = encoder_hidden\n","\n","        loss = 0\n","        for i in range(source.size(1)):\n","            decoder_output, decoder_hidden = decoder(source[:, i], decoder_hidden)\n","            decoder_output = torch.squeeze(decoder_output)\n","            loss += criterion(decoder_output, target[:, i])\n","\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    elapsed_time = time() - start\n","    if epoch % 10 == 0:\n","        print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch, loss.item(), elapsed_time))\n","        \n","model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n","torch.save({\n","    'encoder_model': encoder.state_dict(),\n","    'decoder_model': decoder.state_dict(),\n","}, model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVUbOtyfGLwA"},"source":["## 評価\n","\n","次に，学習したモデルを評価をします．\n","\n","テストデータを2000サンプル生成して，データローダに与えます．\n","\n","ここで，学習時はエンコーダとデコーダのバッチサイズを100としていました．\n","テスト時は１つずつ行いたいので，エンコーダとデコーダを新たに生成し，学習したパラメータをロードします．\n","\n","エンコーダ側に計算したい数字（または記号）を入力して中間情報stateを得ます．\n","デコーダ側に，中間情報stateと開始記号<eos>を入力します．\n","デコーダ側の出力は数字または記号(token)と中間情報です．\n","これらを繰り返しデコーダに入力します．<eos>が出力されたら繰り返しは終了です．\n","\n","出力されたtokenを追加したリストrightを計算結果とします．\n","計算する式(left)を作成した後，evalでその計算結果が正しいかどうかを判定します．\n","\n"]},{"cell_type":"code","metadata":{"id":"OGdS8DWwGC_D"},"source":["batch_size = 1\n","test_data = CalcDataset(data_num = 2000)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n","decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n","\n","model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n","checkpoint = torch.load(model_name)\n","encoder.load_state_dict(checkpoint[\"encoder_model\"])\n","decoder.load_state_dict(checkpoint[\"decoder_model\"])\n","\n","accuracy = 0\n","        \n","# 評価の実行   \n","with torch.no_grad():\n","    for data, label in test_loader:\n","        if use_cuda:\n","            data = data.cuda()\n","\n","        state = encoder(data)\n","\n","        right = []\n","        token = \"<eos>\"\n","        for _ in range(7):\n","            index = word2id[token]\n","            input_tensor = torch.tensor([index], device=device)\n","            output, state = decoder(input_tensor, state)\n","            prob = F.softmax(torch.squeeze(output))\n","            index = torch.argmax(prob.cpu().detach()).item()\n","            token = id2word[index]\n","            if token == \"<eos>\":\n","                break\n","            right.append(token)\n","        right = \"\".join(right)\n","        \n","        x = list(data[0].to('cpu').detach().numpy() )\n","        try:\n","            padded_idx_x = x.index(word2id[\"<pad>\"])\n","        except ValueError:\n","            padded_idx_x = len(x)\n","        left = \"\".join(map(lambda c: str(id2word[c]), x[:padded_idx_x]))\n","\n","        flag = [\"F\", \"T\"][eval(left) == int(right)]\n","        print(\"{:>7s} = {:>4s} :{}\".format(left, right, flag))\n","        if flag == \"T\":\n","            accuracy += 1\n","print(\"Accuracy: {:.2f}\".format(accuracy / len(test_loader)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVqKQdWxGOg5"},"source":["## 課題\n","\n","### 1. 他のリカレントニューラルネットワークを使って精度比較をしてみましょう．\n","\n","**ヒント**\n","\n","その他のネットワークとしては`nn.RNN`や`nn.GRU`があります．\n","\n","また，RNNやGRUにはセル状態がないため，変数`c`を使用しないように注意\n","```\n","self.lstm(embedding, (h, c)) --> self.lstm(embedding, h)\n","```\n","\n","\n","### 2. 足し算だけでなく，色々な四則演算を実装しましょう．\n","**こちらは時間があれば取り組んでみましょう**"]}]}