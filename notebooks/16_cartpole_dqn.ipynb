{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16_cartpole_dqn.ipynb","provenance":[{"file_id":"1Y-JU6S-lB5KG326EGKJxDxfISYP-ByyR","timestamp":1627257977724}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rMvOfX_2IuUo"},"source":["# 深層強化学習によるCart Pole制御\n","\n","---\n","## 目的\n","深層強化学習を用いてCart Pole制御を行う．\n","ここで，Cart Pole制御とは台車に乗っている棒が倒れないように台車を左右に動かすことである．\n","\n","Q-Tableを用いた従来のQ学習による方法とQ-Networkを用いた方法の２種類を行う．\n"]},{"cell_type":"markdown","metadata":{"id":"Log6bIaiDnkV"},"source":["## 準備\n","\n","### Google Colaboratoryの設定確認・変更\n","本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n","**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n","\n","\n","### モジュールの追加インストール\n","下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"]},{"cell_type":"code","metadata":{"id":"piC8yNcqDmyC"},"source":["!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n","\n","!pip -q install gym\n","!pip -q install pyglet\n","!pip -q install pyopengl\n","!pip -q install pyvirtualdisplay"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aKH_oZ9Lb6i"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるopenAI Gym（gym）をインポートする．"]},{"cell_type":"code","metadata":{"id":"z8rIC4r6LFte"},"source":["import numpy as np\n","import gym\n","import gym.spaces\n","\n","import time\n","import math\n","import random\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import collections\n","from itertools import count\n","from PIL import Image\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","# 使用するデバイス（GPU or CPU）の決定\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Use device:\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0V-KflRLV-6"},"source":["# Q-Learning (Q-table)\n","Q-Learningとは、強化学習の手法の中で、TD法を用いた代表的な手法の一つです。最適な行動価値(Q値)を推定するように学習を行いQ値を行動の指針として用いることで最適な行動を行います。Q学習では，全ての状態$s$と行動$a$に対する価値$Q(s, a)$を記録するテーブル(Q-table) を作成します。しかし、初期段階では、各状態と行動に対する正確な行動価値がわからないため、Q-tableを任意の値に初期化します。その後、あらゆる状態の下で行動を行い推定した行動価値を用いてQ-tableを修正していきます。以下に簡略化したQ-Learningの学習方法を記載します。\n","1. ある環境における全ての状態と行動に対する価値(Q値)を記録するためのQ-tableを作成\n","2. Q-tableに記録されたQ値を任意の値で初期化\n","3. $\\epsilon$-greedy法などを用いて環境に対する行動を選択\n","4. 行った行動に対する報酬値とQ-tableに記録されたQ値をもとにQ-tableを更新\n","5. 最適なQ-tableが完成するまで3,4を繰り返す"]},{"cell_type":"markdown","metadata":{"id":"0mdO4rIl6IUQ"},"source":["## 環境の作成\n","今回の実験で使用する環境の作成を行います。 [OpenAI Gym](https://github.com/openai/gym) は、様々な種類の環境を提供しているモジュールです。\n"," \n","今回の実験ではgymで利用できるCartPoleを実行します。\n","まず、gym.make関数で実行したい環境を指定します。\n","その後、reset関数を実行することで、環境を初期化します。また、環境に対する情報を表示することもできます。\\\n","CartPoleは、Cartを操作し、Cartに乗ったPoleを倒さないようにするという環境です。環境における行動は右と左に動くという離散的な行動をもち、状態は連続的な値であらわされたCart位置、Cart速度、Poleの角度、Poleの角速度からなる状態をもっています。"]},{"cell_type":"code","metadata":{"id":"_a46O-q36IUr"},"source":["env = gym.make('CartPole-v0')\n","\n","obs = env.reset()\n","print('observation space:',env.observation_space)\n","print('action space:',env.action_space)\n","print('initial observation',obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnmhZNvQLw4u"},"source":["## Q-tableの作成、離散化処理\n","Q-tableの作成とCartPole環境における状態の離散化処理を行います。\\\n","Q-LearningはQ-tableを用いた学習を行います。しかし、今回使用する環境であるCartPoleはCart位置、Cart速度、Poleの角度、Poleの角速度からなる4次元の状態をもっておりCart速度は(-2.4～2.4)、Cart速度は(-3.0～3.0)、Poleの角度は(-0.5, 0.5)、Poleの角速度は(-2.0, 2.0)の範囲で連続的な数値となっています。Q-Learningでは、任意の大きさのQ-tableを作成しなければいけないため、連続的な数値ではQ-tableを作成することができません。なので、状態を分割し離散的な値に変換することでQ-tableを作成可能とします。\n","今回はnumpyのdigitize関数とlinspace関数を組み合わせて離散化処理を行います。まず、linspace関数で分割数に応じて状態の範囲を区切ります。そして、dizitize関数である値が区切られた範囲でどこの区画に属するのかを返します。これにより、連続的な数値であってもその値がどの区画なのかという数値に変換されるため、離散化された値とすることができます。\n","状態の分割により環境における状態数が決定されるため決定した状態数の大きさに合わせたQ-tableを作成します。\\\n","初めに決定した状態の分割数$x$により、離散化する際の値が変化し、Q-tableの大きさが変化します。CartPoleは4次元の状態を持っているためQ-tableの大きさは $x^4(状態数)×2(actionの数)$となります。"]},{"cell_type":"code","metadata":{"id":"6XI-PzPcLUQh"},"source":["num_dizitized = 10  #状態の分割数\n","\n","def bins(clip_min, clip_max, num):\n","    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","# 各値を離散値に変換\n","def digitize_state(observation):\n","    cart_pos, cart_v, pole_angle, pole_v = observation\n","    digitized = [\n","        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n","        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n","        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n","        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n","    ]\n","    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n","\n","q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**4, env.action_space.n))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bk0eS4l0L9Ym"},"source":["## アクションの選択 ($\\epsilon$-greedy法)\n","アクションの選択方法として$\\epsilon$-greedy法を用いた行動選択の定義を行います。\\\n","Q-Learningでは、Q値をもとに行動選択を行いますが、Q値が大きい行動のみをとり続けることは、局所解へ陥ってしまう問題につながります。そこで、環境の適度な探索と学習によって得られた知見を活用した行動(利用)が重要であり、この探索と利用のトレードオフの問題が強化学習における大きな課題となっています。\n","$\\epsilon$-greedy法は、適度な探索を行う手法であり、確率$\\epsilon$でQ値が最も大きい行動を選択し、確率$1-\\epsilon$で、ランダムな行動を選択するといった手法です。確率$\\epsilon$の値は、学習が進むごとに大きくなっていき、学習初期は探索を行い、徐々に最適な行動のみを選択するようになります。"]},{"cell_type":"code","metadata":{"id":"tZNVYe4RL_Po"},"source":["def get_action_q(next_state, episode):\n","    #徐々に最適行動のみをとる、ε-greedy法\n","    epsilon = 0.5 * (1 / (episode + 1))\n","    if epsilon <= np.random.uniform(0, 1):\n","        next_action = np.argmax(q_table[next_state])\n","    else:\n","        next_action = np.random.choice([0, 1])\n","    return next_action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j3i-zLzYNgNr"},"source":["## Qテーブルの更新\n","Q-tableの更新関数を定義します。\\\n","Q-Learningは、報酬と行動価値をもとに最適なQ-tableを推定していく手法であり、Q-tableの更新にはTD法を用いた更新を行います。ある状態におけるある行動の価値を次状態における最良の行動価値に近似するように更新を行っていきます。更新式としては以下の式となり、この更新式を用いて初期化したQ-tableの更新を行っていきます。あらゆる状態を経験することにより最適なQ-tableが作成されます。\n","\n","\n","\\\n","$$\n","Q(s,a)←Q(s,a)+\\alpha(r+\\gamma \\max_{a'}Q(s',a')-Q(s,a))\n","$$"]},{"cell_type":"code","metadata":{"id":"EYrNsbg1NjMM"},"source":["def update_Qtable(q_table, state, action, reward, next_state):\n","    gamma = 0.99\n","    alpha = 0.5\n","    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n","    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * next_Max_Q)\n","\n","    return q_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdnnYxzvtmi2"},"source":["## 学習に必要な各種パラメータ設定\n","今回の実験に使用する各種パラメータの設定を行います。ここでは、学習における総試行回数などの学習で必要なパラメータの設定を行っています。"]},{"cell_type":"code","metadata":{"id":"ACap98mhtwgl"},"source":["max_number_of_steps = 200  #1試行のstep数\n","num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n","num_episodes = 2000  #総試行回数\n","total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqz_xb-PN15S"},"source":["## Q-Learningメインプログラム\n","Q-Learningのメイン関数です。ここまでの設定に従いQ-Learningを用いて学習します。以下のような流れのプログラムを作成します．\n","1. 環境を初期化・離散化し，初期状態を獲得\n","2. 獲得した初期状態から行動を選択($\\epsilon$-greedy法を用いた行動選択)\n","3. 環境に対してエージェントが行動(遷移情報の獲得)\n","4. 得られた経験を用いてQ-tableを更新\n","5. 指定したステップ数分2～4を繰り返す"]},{"cell_type":"code","metadata":{"id":"LjaOTTeOOpKL"},"source":["for episode in range(num_episodes):  #試行数分繰り返す\n","    # 環境の初期化\n","    observation = env.reset()\n","    state = digitize_state(observation)\n","    action = np.argmax(q_table[state])\n","    episode_reward = 0\n","\n","    for t in range(max_number_of_steps):  #1試行のループ\n","\n","        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n","        observation, reward, done, info = env.step(action)\n","\n","        # 報酬を設定し与える\n","        if done:\n","            if t < 195:\n","                reward = -200  #こけたら罰則\n","            else:\n","                reward = 1  #立ったまま終了時は罰則はなし\n","        else:\n","            reward = 1  #各ステップで立ってたら報酬追加\n","\n","        episode_reward += reward  #報酬を追加\n","\n","        # 離散状態s_{t+1}を求め、Q関数を更新する\n","        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\n","        q_table = update_Qtable(q_table, state, action, reward, next_state)\n","\n","        #  次の行動a_{t+1}を求める \n","        action = get_action_q(next_state, episode)    # a_{t+1} \n","\n","        state = next_state\n","\n","        #終了時の処理\n","        if done and (t%100==0):\n","            print('%d Episode finished after %f time steps / mean %f' %\n","                  (episode, t + 1, total_reward_vec.mean()))\n","            total_reward_vec = np.hstack((total_reward_vec[1:],\n","                                          episode_reward))  #報酬を記録\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zX-V2yIoF1zG"},"source":["## 結果を保存\n","Q-学習を用いて学習したエージェントを確認してみます．\\\n","ここでは，ここでは学習時と同様の処理を行いframesに描画したフレームを順次格納します．学習時と異なるのは，$\\epsilon$-greedy法は用いず常にQ値が最も高い行動を選択します．指定したステップ数分フレームを獲得したら終了します．"]},{"cell_type":"code","metadata":{"id":"0wCnPwgoc8QR"},"source":["# 結果を描画するための設定\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n","\n","\n","frames = []\n","for i in range(3):\n","    done = False\n","    t = 0\n","    observation = env.reset() \n","    state = digitize_state(observation)\n","    action = np.argmax(q_table[state])\n","    episode_reward = 0  \n","    while not done and t < 200:\n","        frames.append(env.render(mode='rgb_array'))\n","        observation, reward, done, info = env.step(action)\n","        state = digitize_state(observation)\n","        action = np.argmax(q_table[state])\n","        t += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ps1P-_Y4F6MA"},"source":["## 結果を描画\n","maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"]},{"cell_type":"code","metadata":{"id":"5h1uicqYCkaZ"},"source":["# 実行結果の表示\n","import matplotlib.pyplot as plt\n","import matplotlib.animation\n","from IPython.display import HTML\n","\n","plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","patch = plt.imshow(frames[0])\n","plt.axis('off')\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WiGWn0U4oV5B"},"source":["## 課題"]},{"cell_type":"markdown","metadata":{"id":"Mqb7BDsXP1bR"},"source":["# Q-Network\n","Q-Networkは、Q-Learningにニューラルネットワークの技術を適用した手法です。\\\n","Q-Learningでは、状態、行動数に合わせたQ-tableを作成し、TD法を用いたQ値の更新を行うことで最適なQ-tableを作成する手法でした。しかし問題点として、CartPoleのような状態数が連続的な値の場合、Q-tableが作成可能となるように状態数の離散化を行わなければいけないという問題点がありました。そこで、Q-Networkではニューラルネットワークを用いた写像関数によって、Q-tableを表現することで、連続的な状態でも直接入力として使用可能としています。ネットワークの出力を各行動に対応するのQ値を出力するように設計し、損失関数を用いた学習により最適なQ関数を近似します。"]},{"cell_type":"markdown","metadata":{"id":"0MpSMZnZPOHL"},"source":["## 環境の作成\n","Q-Learningと同様環境の作成を行います。"]},{"cell_type":"code","metadata":{"id":"OUkpNZYuPM6r"},"source":["env = gym.make('CartPole-v0')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSbxNbZLP87L"},"source":["## ネットワークモデル\n","ネットワークの設計を行います。\\\n","Q-Networkでは、\n","入力は、CartPoleにおけるCart位置、Cart速度、Poleの角度、Poleの各速度の4次元の情報を直接入力とします。全結合層が2層で構成されるネットワークとし、出力は各行動ごとのQ値を出力するため出力層はactionの数として定義します。"]},{"cell_type":"code","metadata":{"id":"4zXvS2kHHSe2"},"source":["class QNetwork(nn.Module):\n","    def __init__(self, input_shape, n_actions):\n","        super(QNetwork, self).__init__()\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(4, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, n_actions)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekd_xLCIQVY8"},"source":["## アクション選択\n","アクションの選択方法として$\\epsilon$-greedy法を用いた行動選択の定義を行います。\\\n","Q-Learningと同様に適度な探索を行うため、$\\epsilon$-greedy法を用いて行動を選択します。確率$\\epsilon$で、ネットワークが出力したQ値の値が最も大きい行動を選択し、確率$1-\\epsilon$でランダムな行動を選択します。"]},{"cell_type":"code","metadata":{"id":"wx1PsYe1QVA1"},"source":["def get_action_qn(next_state, net, episode, device=\"cpu\"):\n","    epsilon = 0.5 * (1 / (episode + 1))\n","    if epsilon <= np.random.uniform(0, 1):\n","        state_a = np.array([next_state], copy=False)\n","        state_v = torch.tensor(state_a).float().to(device)\n","        q_vals_v = net(state_v)\n","        _, act_v = torch.max(q_vals_v, dim=1)\n","        next_action = int(act_v.item())\n","    else:\n","        next_action = np.random.choice([0, 1])\n","\n","    return next_action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWZEuBXNuSXK"},"source":["## 損失関数の計算\n","ネットワークを更新するための損失関数の定義を行います。\\\n","Q-Networkでは、ニューラルネットワークで最適なQ関数を近似します。ニューラルネットワークを用いた学習のため損失関数を設計し、逆伝搬によりネットワークのパラメータを更新していきます。\n","損失関数はQ-Learnigの更新式をもとに設計されていて、以下のような二乗誤差になっています。\n","\n","\\\n","$$\n","L_{\\theta}=(r+\\gamma \\max_{a'}Q_{\\theta_{i}}(s',a')-Q_{\\theta_{i}}(s,a))^{2}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"p9kh56HGuR4y"},"source":["def calc_loss(batch, net, device=\"cpu\"):\n","    states, actions, rewards, dones, next_states = batch\n","\n","    states_v = torch.tensor(states).float().to(device)\n","    next_states_v = torch.tensor(next_states).float().to(device)\n","    actions_v = torch.tensor(actions).to(device)\n","    rewards_v = torch.tensor(rewards).to(device)\n","    done_mask = torch.ByteTensor(dones).to(device)\n","\n","    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","    next_state_values = net(next_states_v).max(1)[0]\n","    next_state_values[done_mask] = 0.0\n","    next_state_values = next_state_values.detach()\n","\n","    expected_state_action_values = next_state_values * GAMMA + rewards_v\n","    return nn.MSELoss()(state_action_values, expected_state_action_values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LY5714Ivsr1_"},"source":["## バッチ処理のためのbuffer\n","バッチ学習のために経験を収納、取り出す関数を定義します。"]},{"cell_type":"code","metadata":{"id":"K8svUw3jsoS-"},"source":["Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n","\n","class Buffer:\n","    def __init__(self, capacity):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def append(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in range(batch_size)])\n","        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n","               np.array(dones, dtype=np.uint8), np.array(next_states)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvbY013-wMbx"},"source":["## 学習に必要な各種パラメータ設定\n","今回の実験に使用する各種パラメータの設定を行います。ここでは、学習における総試行回数などの学習で必要なパラメータの設定を行っています。\n","また、ネットワークの定義や、最適化手法を指定しています。今回の実験では最適化手法はAdamとしています。"]},{"cell_type":"code","metadata":{"id":"qzXWZYdawLJD"},"source":["max_number_of_steps = 200  #1試行のstep数\n","num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n","num_episodes = 2000  #総試行回数\n","\n","total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\n","\n","LEARNING_RATE = 1e-4 #学習率\n","GAMMA = 0.99 #割引率\n","\n","batch_size = 32 #バッチサイズ\n","device = 'cuda:0'\n","train_num = 0\n","\n","net = QNetwork(env.observation_space.shape, env.action_space.n).to(device)\n","optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","buffer = Buffer(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfrffImFQAKv"},"source":["## 学習のメイン関数\n","Q-Learningのメイン関数です。ここまでの設定に従いQ-Newtworkの学習を行います。以下のような流れのプログラムを作成します．\n","1. 環境を初期化・離散化し，初期状態を獲得\n","2. 獲得した初期状態から行動を選択($\\epsilon$-greedy法を用いた行動選択)\n","3. 環境に対してエージェントが行動(遷移情報の獲得)\n","4. バッチ数分経験を得たら損失関数の計算\n","5. 誤差逆伝播法を用いてネットワークの更新\n","6. 指定したステップ数分2～5を繰り返す"]},{"cell_type":"code","metadata":{"id":"k9Y9prBYQFRO"},"source":["for episode in range(num_episodes):  #試行数分繰り返す\n","    # 環境の初期化\n","    observation = env.reset()\n","    state = observation\n","    episode_reward = 0\n","\n","    for t in range(max_number_of_steps):  #1試行のループ\n","        action = get_action_qn(observation, net, episode, device)\n","        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n","        observation, reward, done, info = env.step(action)\n","        next_state = observation\n","\n","        # 報酬を設定し与える\n","        if done:\n","            if t < 195:\n","                reward = -200  #こけたら罰則\n","            else:\n","                reward = 1  #立ったまま終了時は罰則はなし\n","        else:\n","            reward = 1  #各ステップで立ってたら報酬追加\n","\n","        episode_reward += reward  #報酬を追加\n","        \n","        exp = Experience(state, action, reward, done, next_state)\n","        buffer.append(exp)\n","        state = next_state\n","\n","        if train_num > batch_size:\n","            optimizer.zero_grad()\n","            batch = buffer.sample(batch_size)\n","            loss_t = calc_loss(batch, net, device=device)\n","            loss_t.backward()\n","            optimizer.step()\n","        \n","        train_num += 1\n","\n","        #終了時の処理\n","        if done and (t%100==0):\n","            print('%d Episode finished after %f time steps / mean %f' %\n","                  (episode, t + 1, total_reward_vec.mean()))\n","            total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))  #報酬を記録"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3iQMt8MonQI"},"source":["## 結果を保存\n","学習したネットワーク（エージェント）を確認してみます．\\\n","ここでは，学習時と同様の処理を行いframesに描画したフレームを順次格納します．学習時と異なるのは，$\\epsilon$-greedy法は用いず常にQ値が最も高い行動を選択します．指定したステップ数分フレームを獲得したら終了します．"]},{"cell_type":"code","metadata":{"id":"nj4FhGY_omOQ"},"source":["# 結果を描画するための設定\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n","\n","\n","frames = []\n","for i in range(3):\n","    done = False\n","    t = 0\n","    state = env.reset() \n","    episode_reward = 0  \n","    while not done and t < 200:\n","        frames.append(env.render(mode='rgb_array'))\n","        state_a = np.array([state], copy=False)\n","        state_v = torch.tensor(state_a).float().to(device)\n","        q_vals_v = net(state_v)\n","        _, act_v = torch.max(q_vals_v, dim=1)\n","        action = int(act_v.item())\n","        new_state, reward, done, info = env.step(action)\n","        state = new_state\n","        t += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5tJoQekxstF"},"source":["## 描画\n","\n","maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"]},{"cell_type":"code","metadata":{"id":"wFDCxq1GeXl4"},"source":["# 実行結果の表示\n","import matplotlib.pyplot as plt\n","import matplotlib.animation\n","from IPython.display import HTML\n","\n","plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","patch = plt.imshow(frames[0])\n","plt.axis('off')\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttgAMbF5odix"},"source":["# 課題\n","\n","1. Q-Tableを用いた学習方法において，テーブルを作成する際の「状態の分割数」を変えて実験してみましょう．\n","\n","2. 学習のパラメータを変えて実験してみましょう．\n","  * 通常の深層学習のパラメータに加えて，強化学習特有のパラメータとして割引率 `gamma` などがあります．\n","\n","3. 報酬の値を変更して実験してみましょう．\\\n","※ Q-Tableを用いる学習の場合は「Q-Learningメインプログラム」\\\n","Q-Networkの場合は「学習のメイン関数」にある報酬の値を変更します．"]}]}