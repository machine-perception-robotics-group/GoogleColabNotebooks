{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"18_grad_cam.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vQfhYcVF8KPt"},"source":["# CNNの可視化 (Gradient-weighted Class Activation Mapping; Grad-CAM)\n","\n","---\n","\n","## 目的\n","Gradient-weighted Class Activation Mapping (Grad-CAM)の仕組みを理解する.\n","\n","Grad-CAMを用いてCIFAR-10データセットに対するネットワークの判断根拠の可視化を行う．\n","\n","## Gradient-weighted Class Activation Mapping (Grad-CAM)\n","Grad-CAM[1]は，逆伝播時の正値の勾配を用いることでCNNを可視化する手法です．\n","Grad-CAMは，逆伝播時の特定のクラスにおける勾配をGlobal Average Pooling (GAP)[2]により空間方向に対する平均値を求め，各特徴マップに対する重みとします．\n","その後，獲得した重みを各特徴マップに重み付けすることでAttention map を獲得します．\n","02_CAM.ipynbで使用したClass Activation Mapping (CAM)[3]は，ネットワークの一部をGAPに置き換える必要があるため，Attention mapを獲得するためにネットワークを学習させる必要があります．一方で，Grad-CAMはネットワークの順伝播時の特徴マップと逆伝播時の勾配を用いてAttention mapを獲得します．そのため，学習済みの様々なネットワークからAttention map を獲得することができます．\n","\n","\n","<img src=\"https://www.dropbox.com/s/x23sm70ftoo7caa/grad-cam.png?dl=1\" width = 100%>\n"]},{"cell_type":"markdown","metadata":{"id":"V8ehSX6U8KPv"},"source":["## モジュールのインポート\n","プログラムの実行に必要なモジュールをインポートします．\n","今回は，機械学習ライブラリであるPytorchを使用します．\n","PyTorchとは，Python向けのオープンソース機械学習ライブラリで，Facebookに開発されました．"]},{"cell_type":"code","metadata":{"id":"VaNXqInW8KPv"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.backends.cudnn as cudnn\n","import torch.nn as nn\n","import torchsummary\n","from torchvision import datasets, models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4gEy7cGCfz8"},"source":["## GPUの確認\n","GPUを使用した計算が可能かどうかを確認します．\n","下記のコードを実行してGPU情報を確認します． GPUの確認を行うためには，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．\n","\n","`Use CUDA: True`と表示されれば，GPUを使用した計算をPytorchで行うことが可能です． CPUとなっている場合は，上記に記載している手順にしたがって，設定を変更してください．"]},{"cell_type":"code","metadata":{"id":"p9fjeG_U8KP1"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","use_cuda = torch.cuda.is_available()\n","cudnn.benchmark = True\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uanw74k0F9iw"},"source":["下記のコードを実行してGPU情報を確認します．\n","\n"]},{"cell_type":"code","metadata":{"id":"eeUIUazLGGPu"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XzIClJ5k6h4"},"source":["## 使用するデータセット\n","\n","### データセット\n","今回の物体認識では，CIFAR-10データセットを使用します．CIFAR-10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットです．\n","\n","![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"]},{"cell_type":"markdown","metadata":{"id":"xNzvYQL58KP4"},"source":["### データセットのダウンロードと読み込み\n","実験に使用するCIFAR-10データセットを読み込みます．\n","１回の誤差を算出するデータ数 (ミニバッチサイズ) は，64とします．\n","まず，CIFAR-10データセットをダウンロードします．\n","次に，ダウンロードしたデータセットを読み込みます．\n","学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです．"]},{"cell_type":"code","metadata":{"id":"leWJTOIL8KP4"},"source":["batch_size = 64\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Scale(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=20)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=20)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHQfAwb_8KP7"},"source":["### CIFAR-10データセットの表示\n","CIFAR-10データセットに含まれる画像を表示してみます．\n","ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．\n","学習データは5万枚，1つのデータサイズは3x32x32の画像のような形式となっています．これは32x32ピクセルのカラー画像という意味になります．\n","\n"]},{"cell_type":"code","metadata":{"id":"Fod_SRFR8KP8"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","def imshow(img):\n","    npimg = img.numpy()\n","    npimg = ((npimg.transpose((1,2,0))  * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465])  # unnormalize\n","    plt.imshow(npimg)\n","    plt.show()\n","\n","# get some random training images\n","dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images[0:4]))\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DduOi-s18KP_"},"source":["## ネットワークモデルの定義\n","学習済みモデルにはResNet-18を利用して学習します．`pretrained = True`にすると，ImageNetで学習したモデルを利用できます．ここで，ImageNetは1,000クラスのデータセットです．すなわち，ImageNetで学習したResNet-18の出力層のユニット数は1,000になっています．ファインチューニングに利用するCIFAR-10データセットは10クラスなので，出力層のユニット数を変更します．"]},{"cell_type":"code","metadata":{"id":"O0cpWP_w8KP_"},"source":["model = models.resnet18(pretrained=True)\n","print(\"======== Original netowrk architecutre ========\\n\")\n","print(model)\n","\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","print(\"======== Fine-funing netowrk architecutre ========\\n\")\n","print(model)\n","\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFCekfs78KQC"},"source":["## 損失関数と最適化手法の定義\n","学習に使用する損失関数と最適化手法を定義します．\n","各更新において，学習用データと教師データをそれぞれ`inputs`と`targets`とします．\n","学習モデルに`inputs`を与えて，ResNetの出力を取得します．\n","ResNetの出力と教師ラベル`targets`との誤差を`criterion`で算出します．\n","また，認識精度も算出します．\n","そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n","最適化手法には，確率的勾配降下法 (stochastic gradient descent: SGD) を用いて学習します．\n","\n","最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．\n"]},{"cell_type":"code","metadata":{"id":"rntVJhx98KQC"},"source":["import torch.optim as optim\n","from torch.optim import SGD, lr_scheduler\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","#scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1) # 50 < 75\n","\n","# モデルの情報を表示\n","torchsummary.summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4NRFd6i8KQF"},"source":["## 学習\n","学習エポック数を10とします．\n","CIFAR-10データセットの学習データサイズを取得し，1エポック内における更新回数を求めます．\n","1エポック学習するごとに学習したモデルを評価し，最も精度の高いモデルが保存されます．"]},{"cell_type":"code","metadata":{"id":"nnMVTad98KQG"},"source":["epochs = 10\n","best_acc = 0  # best test accuracy\n","\n","# Training\n","for epoch in range(epochs):\n","    #scheduler.step()\n","    train_running_loss = 0.0\n","    train_running_acc = 0.0\n","\n","    # training\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    count = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        # print statistics\n","        train_running_loss += loss\n","        train_running_acc += 100.*correct/total\n","        count += 1\n","\n","    print('[Epoch %d] Train Loss: %.5f | Train Acc: %.3f%%'\n","                  % (epoch + 1, train_loss/count, train_running_acc/count))\n","    \n","    # testing\n","    model.eval() \n","    with torch.no_grad():\n","        test_running_loss = 0.0\n","        test_running_acc = 0.0\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        count = 0\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            # print statistics\n","            test_running_loss += loss\n","            test_running_acc += 100.*correct/total\n","            count += 1\n","\n","        print('Test Loss: %.5f | Test Acc: %.3f%%'\n","                      % (test_loss/count, test_running_acc/count))\n","        \n","    # save model\n","    if test_running_acc/count > best_acc:\n","        best_acc = max(test_running_acc/count, best_acc)\n","        PATH = './cifar_net.pth'\n","        torch.save(model.state_dict(), PATH)\n","    \n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlOo-rzY8KQL"},"source":["##テスト\n","学習したネットワークのテストデータに対する認識率の確認を行います．まず，学習したネットワークを評価するために保存したモデルをロードします．"]},{"cell_type":"code","metadata":{"id":"WSa0ATFj8KQP"},"source":["PATH = './cifar_net.pth'\n","model.load_state_dict(torch.load(PATH))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VwWfJO358KQS"},"source":["次に，学習したネットワークを用いて，テストデータに対する認識率の確認を行います．\n","`model.eval()`を適用することで，ネットワーク演算を評価モードへ変更します． これにより，学習時と評価時で挙動が異なる演算（dropout等）を変更することが可能です． また，`torch.no_grad()`を適用することで，学習時には必要になる勾配情報を保持することなく演算を行います．"]},{"cell_type":"code","metadata":{"id":"ij38kQBP8KQS"},"source":["# testing\n","model.eval() \n","with torch.no_grad():\n","    test_running_loss = 0.0\n","    test_running_acc = 0.0\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    count = 0\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        test_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        # print statistics\n","        test_running_loss += loss\n","        test_running_acc += 100.*correct/total\n","        count += 1\n","\n","    print('Test Loss: %.5f | Test Acc: %.3f%%'\n","                  % (test_loss/count, test_running_acc/count))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FMFN4AkzRM5Y"},"source":["##Grad-CAMによるAttention mapの可視化\n","Grad-CAMを利用するために必要なツールをインストールします．\n","Grad-CAMは，`pytorch-gradcam`というツールをインストールすることで簡単に利用することができます．\n"]},{"cell_type":"code","metadata":{"id":"veTKG-vmQsdV"},"source":["!pip install pytorch-gradcam"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWkM1AaK829Q"},"source":["Grad-CAMによりAttention mapを可視化して，ネットワークの判断根拠を確認してみます． 再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます． pred (prediction) は認識結果，conf (confidence) は認識結果に対する信頼度を示しています．"]},{"cell_type":"code","metadata":{"id":"f6cIbDMZQWaF"},"source":["from gradcam.utils import visualize_cam\n","from gradcam import GradCAM\n","import cv2\n","import numpy as np\n","\n","transform_test = transforms.Compose([\n","    transforms.Scale(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","])\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=False, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n","                                         shuffle=True, num_workers=20)\n","\n","classes_list = ['plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Grad-CAM\n","target_layer = model.layer4 # ex., layer1, layer2, layer3, layer3[1], layer4[0]\n","gradcam = GradCAM(model, target_layer)\n","\n","softmax = nn.Softmax(dim=1)\n","\n","def save_gradcam(gcam, raw_image):\n","    h, w, _ = raw_image.shape\n","    #gcam = cv2.resize(gcam, (w, h))\n","    gcam = gcam * 255.0\n","    gcam = np.uint8(gcam)\n","    gcam = gcam.transpose((1, 2, 0))\n","    #gcam = cv2.applyColorMap(gcam, cv2.COLORMAP_JET)\n","    #jet_map = cv2.addWeighted(raw_image, 0.4, gcam, 0.6, 0)\n","    v_list.append(raw_image)\n","    att_list.append(gcam)\n","\n","for batch_idx, (inputs, targets) in enumerate(testloader):\n","    inputs, targets = inputs.to(device), targets.to(device)\n","    outputs = model(inputs)\n","    outputs = softmax(outputs)\n","    conf_data = outputs.data.topk(1, 1, True, True)\n","    _, predicted = outputs.max(1)\n","    d_inputs = inputs.data.cpu()\n","    d_inputs = d_inputs.numpy()\n","    in_b, in_c, in_y, in_x = inputs.shape\n","\n","    v_list = []\n","    att_list = []\n","    for i in range(in_b):\n","        input = inputs[i,:,:,:]\n","        #input = torch.unsqueeze(input, dim=0)\n","        normed_torch_img = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])(input)[None]\n","\n","        v_img = d_inputs[i,:,:,:]\n","        v_img = v_img.transpose(1, 2, 0) * 255\n","        v_img = np.uint8(v_img)\n","\n","        mask, _ = gradcam(normed_torch_img)\n","        heatmap, result = visualize_cam(mask, input)\n","\n","        save_gradcam(result, v_img)\n","    break\n","\n","# Show attention map\n","cols = 8\n","rows = 1\n","\n","fig = plt.figure(figsize=(14, 3.0))\n","plt.title('Input image')\n","plt.axis(\"off\")\n","for r in range(rows):\n","    for c in range(cols):\n","        cls = targets[c].item()\n","        ax = fig.add_subplot(r+1, cols, c+1)\n","        plt.title('{}'.format(classes_list[cls]))\n","        ax.imshow(v_list[cols * r + c])\n","        ax.set_axis_off()\n","plt.show()\n","\n","fig = plt.figure(figsize=(14, 3.5))\n","plt.title('Attention map')\n","plt.axis(\"off\")\n","for r in range(rows):\n","    for c in range(cols):\n","        pred = predicted[c].item()\n","        conf = conf_data[0][c].item()\n","        ax = fig.add_subplot(r+1, cols, c+1)\n","        ax.imshow(att_list[cols * r + c])\n","        plt.title('pred: {}\\nconf: {:.2f}'.format(classes_list[pred], conf))\n","        ax.set_axis_off()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YjJ48SKsd3dp"},"source":["#課題\n","1. Attention mapを可視化する層を変更して，Attention mapの変化を確認してみましょう．\n"]},{"cell_type":"code","metadata":{"id":"bapCu45reINz"},"source":["#ここにコードを書く"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2B4F1QjBpa9"},"source":["# 参考文献\n","- [1] S. Ramprasaath, R., C. Michael, D. Abhishek,\n","V. Ramakrishna, P. Devi, and B.\n","Dhruv, \"Grad-CAM: Visual explanations from deep networks\n","via gradient-based localization\". In International Conference\n","on Computer Vision, pp. 618–626, 2017.\n","\n","- [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n","and A. Torralba, \"Learning deep features for discriminative\n","localization\". In 2016 IEEE Conference on Computer\n","Vision and Pattern Recognition, pp. 2921–2929, 2016.\n","\n","- [2] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n","In 2nd International Conference on Learning Representations,\n","Banff, AB, Canada, April 14-16, 2014, Conference\n","Track Proceedings, 2014."]}]}