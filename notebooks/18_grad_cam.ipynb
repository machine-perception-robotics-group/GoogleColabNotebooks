{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "18_grad_cam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfhYcVF8KPt"
      },
      "source": [
        "# CNNの可視化 (Gradient-weighted Class Activation Mapping; Grad-CAM)\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "Gradient-weighted Class Activation Mapping (Grad-CAM)の仕組みを理解する.\n",
        "\n",
        "Grad-CAMを用いてCIFAR-10データセットに対するネットワークの判断根拠の可視化を行う．\n",
        "\n",
        "## Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
        "Grad-CAM[1]は，逆伝播時の正値の勾配を用いることでCNNを可視化する手法です．\n",
        "Grad-CAMは，逆伝播時の特定のクラスにおける勾配をGlobal Average Pooling (GAP)[2]により空間方向に対する平均値を求め，各特徴マップに対する重みとします．\n",
        "その後，獲得した重みを各特徴マップに重み付けすることでAttention map を獲得します．\n",
        "02_CAM.ipynbで使用したClass Activation Mapping (CAM)[3]は，ネットワークの一部をGAPに置き換える必要があるため，Attention mapを獲得するためにネットワークを学習させる必要があります．一方で，Grad-CAMはネットワークの順伝播時の特徴マップと逆伝播時の勾配を用いてAttention mapを獲得します．そのため，学習済みの様々なネットワークからAttention map を獲得することができます．\n",
        "\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/x23sm70ftoo7caa/grad-cam.png?dl=1\" width = 100%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwB6Pj1BoGRo"
      },
      "source": [
        "## pytorch-gradcamのインストール\n",
        "\n",
        "Grad-CAMを利用するために必要なツールをインストールします．\n",
        "Grad-CAMは，`pytorch-gradcam`というツールをインストールすることで簡単に利用することができます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmCnC2cqoGpC"
      },
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ehSX6U8KPv"
      },
      "source": [
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．\n",
        "今回はPyTorchのライブラリに加えて，\n",
        "上でインストールしたpytorch-gradcam (`gradcam`) もインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaNXqInW8KPv"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "from gradcam import GradCAM\n",
        "from gradcam.utils import visualize_cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4gEy7cGCfz8"
      },
      "source": [
        "## GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "下記のコードを実行してGPU情報を確認します． GPUの確認を行うためには，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．\n",
        "\n",
        "`Use CUDA: True`と表示されれば，GPUを使用した計算をPytorchで行うことが可能です． CPUとなっている場合は，上記に記載している手順にしたがって，設定を変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9fjeG_U8KP1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Use Device:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XzIClJ5k6h4"
      },
      "source": [
        "## 使用するデータセット\n",
        "\n",
        "### データセット\n",
        "今回の物体認識では，CIFAR-10データセットを使用します．CIFAR-10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットです．\n",
        "\n",
        "![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNzvYQL58KP4"
      },
      "source": [
        "### データセットのダウンロードと読み込み\n",
        "実験に使用するCIFAR-10データセットを読み込みます．\n",
        "１回の誤差を算出するデータ数 (ミニバッチサイズ) は，64とします．\n",
        "まず，CIFAR-10データセットをダウンロードします．\n",
        "次に，ダウンロードしたデータセットを読み込みます．\n",
        "学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWJTOIL8KP4"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Scale(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=20)\n",
        "\n",
        "classes_list = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFCekfs78KQC"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "学習済みモデルにはResNet-18を利用して学習します．`pretrained = True`にすると，ImageNetで学習したモデルを利用できます．ここで，ImageNetは1,000クラスのデータセットです．すなわち，ImageNetで学習したResNet-18の出力層のユニット数は1,000になっています．ファインチューニングに利用するCIFAR-10データセットは10クラスなので，出力層のユニット数を変更します．\n",
        "\n",
        "\n",
        "## 損失関数と最適化手法の定義\n",
        "学習に使用する損失関数と最適化手法を定義します．\n",
        "各更新において，学習用データと教師データをそれぞれ`inputs`と`targets`とします．\n",
        "学習モデルに`inputs`を与えて，ResNetの出力を取得します．\n",
        "ResNetの出力と教師ラベル`targets`との誤差を`criterion`で算出します．\n",
        "また，認識精度も算出します．\n",
        "そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n",
        "最適化手法には，確率的勾配降下法 (stochastic gradient descent: SGD) を用いて学習します．\n",
        "\n",
        "最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rntVJhx98KQC"
      },
      "source": [
        "# ネットワークモデルの定義\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# 損失関数と最適化手法の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# モデルと損失関数をGPU演算へ移動\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4NRFd6i8KQF"
      },
      "source": [
        "## 学習とテスト\n",
        "学習エポック数を3とします．\n",
        "CIFAR-10データセットの学習データサイズを取得し，1エポック内における更新回数を求めます．\n",
        "1エポック学習するごとに学習したモデルを評価し，最も精度の高いモデルが保存されます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnMVTad98KQG"
      },
      "source": [
        "epochs = 3\n",
        "best_acc = 0  # best test accuracy\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # training\n",
        "    model.train()\n",
        "    train_loss, train_running_acc = 0.0, 0.0\n",
        "    correct, total, count = 0, 0, 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # print statistics\n",
        "        train_running_acc += 100.*correct/total\n",
        "        count += 1\n",
        "\n",
        "    print('[Epoch %d] Train Loss: %.5f | Train Acc: %.3f%%'\n",
        "                  % (epoch + 1, train_loss/count, train_running_acc/count))\n",
        "\n",
        "    # testing\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        test_loss, test_running_acc = 0.0, 0.0\n",
        "        correct, total, count = 0, 0, 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # print statistics\n",
        "            test_running_acc += 100.*correct/total\n",
        "            count += 1\n",
        "\n",
        "        print('Test Loss: %.5f | Test Acc: %.3f%%'\n",
        "                      % (test_loss/count, test_running_acc/count))\n",
        "\n",
        "    # save model\n",
        "    if test_running_acc/count > best_acc:\n",
        "        best_acc = max(test_running_acc/count, best_acc)\n",
        "        torch.save(model.state_dict(), './cifar_net.pth')\n",
        "        print(\"Save best model ...\")\n",
        "    \n",
        "print('Finished Training')\n",
        "print(\"Best Test Accuracy: %.3f%%\" % best_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1HF5M4jqMof"
      },
      "source": [
        "## 学習済みモデルの読み込み\n",
        "\n",
        "Grad-CAMによる可視化を行います．\n",
        "\n",
        "まずはじめに，上の学習で最も精度の良かった時のネットワークモデルを用意します．\n",
        "\n",
        "上の学習で得られた最高精度のネットワークモデルのパラメータを読み込みます．\n",
        "まず，`torch.load()`で指定したファイルを読み込み，ファイル内のデータを返します．\n",
        "そして読み込んだデータを`model.load_state_dict()`で`model`へ読み込むことで，保存したネットワークパラメータに沖狩ることができます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSa0ATFj8KQP"
      },
      "source": [
        "model.load_state_dict(torch.load('./cifar_net.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMFN4AkzRM5Y"
      },
      "source": [
        "##Grad-CAMによるAttention mapの獲得\n",
        "\n",
        "Grad-CAMによりAttention mapを可視化して，ネットワークの判断根拠を確認してみます． 再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます． pred (prediction) は認識結果，conf (confidence) は認識結果に対する信頼度を示しています．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6cIbDMZQWaF"
      },
      "source": [
        "# Grad-CAMで可視化する層の指定\n",
        "# (ex., layer1, layer2, layer3, layer3[1], layer4[0])\n",
        "target_layer = model.layer4\n",
        "\n",
        "# Grad-CAMを計算するためのモデルの準備\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "# テストデータを入力し，認識結果を獲得する\n",
        "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    conf_data = outputs.data.topk(k=1, dim=1, largest=True, sorted=True)\n",
        "    _, predicted = outputs.max(1)\n",
        "    d_inputs = inputs.data.cpu().numpy()\n",
        "    break\n",
        "\n",
        "# 得られたAttention mapデータを保存するためのリストを用意\n",
        "v_list = []\n",
        "att_list = []\n",
        "\n",
        "# 上の認識に使用した画像データを1枚ずつ読み込んでGrad-CAMを計算\n",
        "for i in range(inputs.shape[0]):\n",
        "    # 画像データを1枚選択\n",
        "    input = inputs[i:i+1,:,:,:]\n",
        "\n",
        "    mask, _ = gradcam(input)\n",
        "    _, result = visualize_cam(mask, input)\n",
        "\n",
        "    v_img = d_inputs[i,:,:,:]\n",
        "    v_img = v_img.transpose(1, 2, 0) * 255\n",
        "    v_img = np.uint8(v_img)\n",
        "    v_list.append(v_img)\n",
        "\n",
        "    result = result * 255.0\n",
        "    result = np.uint8(result)\n",
        "    result = result.transpose((1, 2, 0))\n",
        "    att_list.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC7E7RRiuhiv"
      },
      "source": [
        "## Attention mapの表示\n",
        "\n",
        "上のプログラムで獲得したAttention mapをmatplotlibを用いて表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EVrgzZaugyd"
      },
      "source": [
        "# 行・列の数を指定\n",
        "cols, rows = 8, 1\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.0))\n",
        "plt.title('Input image')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        cls = targets[c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        plt.title('{}'.format(classes_list[cls]))\n",
        "        ax.imshow(v_list[cols * r + c])\n",
        "        ax.set_axis_off()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3.5))\n",
        "plt.title('Attention map')\n",
        "plt.axis(\"off\")\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        pred = predicted[c].item()\n",
        "        conf = conf_data[0][c].item()\n",
        "        ax = fig.add_subplot(r+1, cols, c+1)\n",
        "        ax.imshow(att_list[cols * r + c])\n",
        "        plt.title('pred: {}\\nconf: {:.2f}'.format(classes_list[pred], conf))\n",
        "        ax.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjJ48SKsd3dp"
      },
      "source": [
        "#課題\n",
        "1. Attention mapを可視化する層を変更して，Attention mapの変化を確認してみましょう．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2B4F1QjBpa9"
      },
      "source": [
        "# 参考文献\n",
        "- [1] S. Ramprasaath, R., C. Michael, D. Abhishek,\n",
        "V. Ramakrishna, P. Devi, and B.\n",
        "Dhruv, \"Grad-CAM: Visual explanations from deep networks\n",
        "via gradient-based localization\". In International Conference\n",
        "on Computer Vision, pp. 618–626, 2017.\n",
        "\n",
        "- [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n",
        "and A. Torralba, \"Learning deep features for discriminative\n",
        "localization\". In 2016 IEEE Conference on Computer\n",
        "Vision and Pattern Recognition, pp. 2921–2929, 2016.\n",
        "\n",
        "- [2] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n",
        "In 2nd International Conference on Learning Representations,\n",
        "Banff, AB, Canada, April 14-16, 2014, Conference\n",
        "Track Proceedings, 2014."
      ]
    }
  ]
}